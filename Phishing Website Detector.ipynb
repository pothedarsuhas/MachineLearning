{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set predictions:\n",
      " [ 1 -1 -1 ... -1 -1  1]\n",
      "accuracy of KNN:  0.9529695507989147\n",
      "misclassfied elements\n",
      "(array([   4,   24,   98,  106,  130,  158,  201,  204,  206,  222,  238,\n",
      "        252,  305,  308,  317,  328,  347,  352,  370,  406,  469,  503,\n",
      "        509,  515,  518,  526,  536,  607,  634,  636,  649,  665,  667,\n",
      "        702,  719,  722,  725,  813,  827,  871,  893,  896,  944,  947,\n",
      "        988, 1021, 1023, 1041, 1045, 1046, 1057, 1058, 1081, 1110, 1132,\n",
      "       1155, 1157, 1189, 1201, 1203, 1238, 1252, 1274, 1279, 1307, 1330,\n",
      "       1399, 1402, 1419, 1466, 1467, 1477, 1483, 1544, 1545, 1557, 1562,\n",
      "       1571, 1609, 1622, 1661, 1671, 1712, 1721, 1747, 1752, 1754, 1755,\n",
      "       1778, 1787, 1810, 1840, 1854, 1883, 1940, 1941, 1948, 2037, 2082,\n",
      "       2138, 2140, 2162, 2216, 2266, 2276, 2338, 2339, 2361, 2381, 2385,\n",
      "       2391, 2393, 2449, 2478, 2507, 2526, 2534, 2585, 2630, 2631, 2635,\n",
      "       2706, 2740, 2760, 2776, 2782, 2803, 2808, 2817, 2822, 2826, 2828,\n",
      "       2856, 2857, 2863, 2894, 2912, 2926, 2928, 2937, 2950, 2967, 2996,\n",
      "       3041, 3060, 3066, 3094, 3103, 3126, 3136, 3162, 3199, 3231, 3235,\n",
      "       3298, 3310]),)\n",
      "misclassfied elements count:  156\n",
      "######################################\n",
      "accuracy of DT: 0.967741935483871\n",
      "misclassfied elements\n",
      "(array([   6,   41,   53,   99,  114,  158,  201,  206,  237,  252,  257,\n",
      "        317,  328,  406,  434,  451,  518,  521,  661,  665,  667,  677,\n",
      "        722,  725,  779,  831,  944,  972,  986,  999, 1016, 1021, 1046,\n",
      "       1057, 1074, 1081, 1084, 1155, 1189, 1201, 1255, 1279, 1287, 1301,\n",
      "       1355, 1434, 1467, 1483, 1544, 1545, 1553, 1702, 1712, 1715, 1721,\n",
      "       1747, 1752, 1754, 1763, 1810, 1814, 1840, 1842, 1953, 1984, 1998,\n",
      "       2004, 2077, 2083, 2124, 2145, 2205, 2208, 2266, 2361, 2373, 2381,\n",
      "       2385, 2386, 2393, 2431, 2449, 2464, 2534, 2535, 2552, 2571, 2585,\n",
      "       2631, 2633, 2667, 2776, 2822, 2866, 2894, 2937, 2967, 3041, 3049,\n",
      "       3066, 3090, 3094, 3136, 3162, 3199, 3298, 3313]),)\n",
      "misclassfied elements count:  107\n",
      "######################################\n",
      "accuracy of DT Bagging: 0.9740729574917094, OOB accuracy: 0.9648442548791522\n",
      "misclassfied elements\n",
      "(array([   1,   99,  114,  158,  201,  206,  240,  252,  257,  317,  328,\n",
      "        406,  451,  482,  484,  503,  509,  518,  591,  649,  665,  667,\n",
      "        722,  725,  742,  831,  944,  947,  972,  986,  999, 1021, 1027,\n",
      "       1046, 1057, 1081, 1155, 1189, 1201, 1279, 1434, 1436, 1467, 1482,\n",
      "       1483, 1544, 1545, 1546, 1712, 1747, 1752, 1754, 1810, 1998, 2083,\n",
      "       2124, 2138, 2145, 2266, 2361, 2381, 2385, 2393, 2449, 2534, 2571,\n",
      "       2585, 2607, 2631, 2657, 2776, 2822, 2850, 2894, 2937, 2950, 2967,\n",
      "       2996, 3013, 3066, 3136, 3162, 3199, 3298, 3313, 3315]),)\n",
      "misclassfied elements count:  86\n",
      "######################################\n",
      "K Nearest Neighbours : 0.9529695507989147\n",
      "Classification Tree : 0.967741935483871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.967741935483871\n",
      "misclassfied elements\n",
      "(array([   6,   24,   41,   53,   99,  130,  158,  201,  204,  206,  222,\n",
      "        237,  238,  252,  257,  305,  317,  328,  370,  406,  434,  469,\n",
      "        509,  518,  521,  607,  661,  665,  667,  702,  719,  722,  725,\n",
      "        813,  896,  944,  972,  986,  999, 1016, 1021, 1041, 1046, 1057,\n",
      "       1074, 1081, 1084, 1132, 1155, 1189, 1201, 1252, 1274, 1279, 1287,\n",
      "       1399, 1434, 1466, 1467, 1483, 1544, 1545, 1562, 1671, 1702, 1712,\n",
      "       1715, 1721, 1747, 1752, 1754, 1810, 1840, 1842, 1854, 1883, 1941,\n",
      "       1948, 1953, 1998, 2004, 2037, 2077, 2082, 2083, 2124, 2140, 2162,\n",
      "       2205, 2216, 2266, 2276, 2339, 2361, 2381, 2385, 2393, 2449, 2464,\n",
      "       2478, 2507, 2526, 2534, 2552, 2585, 2630, 2631, 2633, 2776, 2803,\n",
      "       2808, 2822, 2826, 2828, 2856, 2857, 2866, 2894, 2926, 2937, 2967,\n",
      "       3041, 3060, 3066, 3090, 3094, 3126, 3136, 3162, 3199, 3298, 3310]),)\n",
      "misclassfied elements count:  132\n",
      "######################################\n",
      "Logistic Regression: 0.9276454627675611\n",
      "misclassfied elements\n",
      "(array([   1,   24,   29,   38,   39,   41,   51,  114,  123,  130,  158,\n",
      "        166,  201,  206,  222,  226,  232,  235,  252,  258,  261,  288,\n",
      "        317,  347,  361,  406,  451,  469,  470,  482,  495,  498,  503,\n",
      "        509,  518,  536,  558,  583,  591,  634,  635,  649,  665,  717,\n",
      "        722,  725,  758,  771,  819,  827,  828,  853,  862,  896,  937,\n",
      "        944,  947,  953,  957,  967,  970,  971,  994, 1016, 1021, 1039,\n",
      "       1041, 1045, 1057, 1071, 1081, 1084, 1089, 1110, 1115, 1155, 1189,\n",
      "       1200, 1203, 1216, 1223, 1236, 1238, 1248, 1250, 1252, 1260, 1274,\n",
      "       1279, 1298, 1323, 1330, 1338, 1349, 1376, 1399, 1425, 1443, 1466,\n",
      "       1467, 1477, 1483, 1484, 1491, 1499, 1501, 1510, 1521, 1545, 1551,\n",
      "       1557, 1562, 1580, 1607, 1609, 1622, 1639, 1640, 1661, 1671, 1675,\n",
      "       1702, 1712, 1721, 1728, 1744, 1748, 1752, 1754, 1755, 1763, 1773,\n",
      "       1778, 1787, 1802, 1810, 1814, 1819, 1839, 1840, 1854, 1860, 1867,\n",
      "       1910, 1914, 1923, 1925, 1932, 1940, 1948, 1981, 2018, 2056, 2088,\n",
      "       2104, 2138, 2140, 2145, 2228, 2234, 2250, 2264, 2266, 2269, 2291,\n",
      "       2317, 2338, 2357, 2361, 2362, 2365, 2381, 2387, 2393, 2449, 2480,\n",
      "       2526, 2534, 2568, 2571, 2576, 2614, 2631, 2635, 2657, 2691, 2697,\n",
      "       2706, 2708, 2714, 2722, 2744, 2754, 2760, 2776, 2782, 2793, 2803,\n",
      "       2808, 2817, 2826, 2847, 2850, 2851, 2857, 2894, 2899, 2901, 2912,\n",
      "       2919, 2920, 2928, 2937, 2942, 2963, 2967, 2983, 2985, 2994, 2996,\n",
      "       2998, 3041, 3042, 3049, 3058, 3066, 3099, 3110, 3136, 3162, 3163,\n",
      "       3177, 3194, 3199, 3208, 3209, 3235, 3238, 3284, 3298]),)\n",
      "misclassfied elements count:  240\n",
      "######################################\n",
      "Random Forest Classifier Accuracy: 0.9764847753994573\n",
      "misclassfied elements\n",
      "(array([   1,  114,  158,  201,  252,  261,  328,  451,  509,  518,  591,\n",
      "        649,  665,  667,  722,  831,  962,  972, 1021, 1046, 1057, 1081,\n",
      "       1084, 1155, 1189, 1201, 1279, 1434, 1467, 1483, 1544, 1545, 1557,\n",
      "       1671, 1702, 1712, 1721, 1747, 1752, 1754, 1755, 1810, 1959, 1998,\n",
      "       2083, 2124, 2138, 2207, 2266, 2317, 2350, 2361, 2381, 2385, 2393,\n",
      "       2449, 2534, 2571, 2631, 2708, 2776, 2821, 2822, 2850, 2857, 2863,\n",
      "       2894, 2937, 2950, 2967, 2996, 3041, 3066, 3136, 3162, 3199, 3298,\n",
      "       3302]),)\n",
      "misclassfied elements count:  78\n",
      "BEST MODEL\n",
      "######################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "accuracy of KNN  :  0.9529695507989147\n",
      "accuracy of DT  :  0.967741935483871\n",
      "accuracy of Bagging DT  :  0.9740729574917094\n",
      "oob accuracy of Bagging DT  :  0.9648442548791522\n",
      "accuracy of K Nearest Neighbours in Voting  :  0.9529695507989147\n",
      "accuracy of Classification Tree in Voting  :  0.967741935483871\n",
      "accuracy of Voting classifiers collectively  :  0.967741935483871\n",
      "accuracy of Logistic Regression  :  0.9276454627675611\n",
      "accuracy of Random Forest Classifier  :  0.9764847753994573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Dict of accuracies\n",
    "d = {}\n",
    "\n",
    "p = pd.read_csv(\"phishing.csv\")\n",
    "#p.info()\n",
    "#print(type(p))\n",
    "\n",
    "p.columns = ['having_IPhaving_IP_Address',\n",
    "'URLURL_Length',\n",
    "'Shortining_Service',\n",
    "'having_At_Symbol',\n",
    "'double_slash_redirecting',\n",
    "'Prefix_Suffix',\n",
    "'having_Sub_Domain',\n",
    "'SSLfinal_State',\n",
    "'Domain_registeration_length',\n",
    "'Favicon',\n",
    "'port',\n",
    "'HTTPS_token',\n",
    "'Request_URL',\n",
    "'URL_of_Anchor',\n",
    "'Links_in_tags',\n",
    "'SFH',\n",
    "'Submitting_to_email',\n",
    "'Abnormal_URL',\n",
    "'Redirect',\n",
    "'on_mouseover',\n",
    "'RightClick',\n",
    "'popUpWidnow',\n",
    "'Iframe',\n",
    "'age_of_domain',\n",
    "'DNSRecord',\n",
    "'web_traffic',\n",
    "'Page_Rank',\n",
    "'Google_Index',\n",
    "'Links_pointing_to_page',\n",
    "'Statistical_report',\n",
    "'Result']\n",
    "target = p[\"Result\"].T\n",
    "\n",
    "data = p.drop('Result', axis = 1)\n",
    "#print(data)\n",
    "#print(target)\n",
    "\n",
    "#KNN classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(data, target, test_size=0.3,random_state=21, stratify=target)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Test set predictions:\\n {}\".format(y_pred))\n",
    "print(\"accuracy of KNN: \",knn.score(X_test, y_test))\n",
    "\n",
    "d['accuracy of KNN'] = knn.score(X_test, y_test)\n",
    "\n",
    "#len(X_test)\n",
    "\n",
    "y_test = np.asarray(y_test)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "misclassified = np.where(y_test != y_pred)\n",
    "print(\"misclassfied elements\")\n",
    "print(misclassified)\n",
    "print(\"misclassfied elements count: \", len(misclassified[0]) )\n",
    "print(\"######################################\")\n",
    "##Decision Trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=25, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"accuracy of DT: {}\".format(acc))\n",
    "\n",
    "d['accuracy of DT'] = acc\n",
    "\n",
    "misclassified = np.where(y_test != y_pred_dt)\n",
    "print(\"misclassfied elements\")\n",
    "print(misclassified)\n",
    "print(\"misclassfied elements count: \", len(misclassified[0]) )\n",
    "print(\"######################################\")\n",
    "## Ensemble bagging\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt_b = DecisionTreeClassifier(random_state=1, max_depth = 25)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt_b, \n",
    "                       n_estimators=50,\n",
    "                       oob_score=True,\n",
    "                       random_state=1)\n",
    "\n",
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred_dt_b = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred_dt_b)\n",
    "\n",
    "d['accuracy of Bagging DT'] = acc_test\n",
    "\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "d['oob accuracy of Bagging DT'] = acc_oob\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('accuracy of DT Bagging: {}, OOB accuracy: {}'.format(acc_test, acc_oob))\n",
    "\n",
    "misclassified = np.where(y_test != y_pred_dt_b)\n",
    "print(\"misclassfied elements\")\n",
    "print(misclassified)\n",
    "print(\"misclassfied elements count: \", len(misclassified[0]) )\n",
    "print(\"######################################\")\n",
    "\n",
    "\n",
    "##Ensemble Voting\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SEED = 1\n",
    "\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=SEED, max_depth = 25)\n",
    "\n",
    "\n",
    "classifiers = [ ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
    "for clf_name, clf in classifiers:  \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)  \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)  \n",
    "    d['accuracy of'+ ' ' + clf_name +' in Voting'] = accuracy\n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{} : {}'.format(clf_name, accuracy))\n",
    "    \n",
    "\n",
    "# Import VotingCLassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc \n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred_vc = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {}'.format(accuracy))\n",
    "\n",
    "d['accuracy of Voting classifiers collectively'] = accuracy\n",
    "\n",
    "misclassified = np.where(y_test != y_pred_vc)\n",
    "print(\"misclassfied elements\")\n",
    "print(misclassified)\n",
    "print(\"misclassfied elements count: \", len(misclassified[0]) )\n",
    "print(\"######################################\")\n",
    "\n",
    "##Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C = 100, solver='newton-cg', max_iter = 25)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred_LoR = logreg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_LoR)\n",
    "print('Logistic Regression: {}'.format(accuracy))\n",
    "\n",
    "d['accuracy of Logistic Regression'] = accuracy\n",
    "\n",
    "misclassified = np.where(y_test != y_pred_LoR)\n",
    "print(\"misclassfied elements\")\n",
    "print(misclassified)\n",
    "print(\"misclassfied elements count: \", len(misclassified[0]) )\n",
    "print(\"######################################\")\n",
    "\n",
    "##Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC = RandomForestClassifier(n_estimators = 30, random_state = 13)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred_RFC = RFC.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_RFC)\n",
    "print('Random Forest Classifier Accuracy: {}'.format(accuracy))\n",
    "\n",
    "d['accuracy of Random Forest Classifier'] = accuracy\n",
    "\n",
    "misclassified = np.where(y_test != y_pred_RFC)\n",
    "print(\"misclassfied elements\")\n",
    "print(misclassified)\n",
    "print(\"misclassfied elements count: \", len(misclassified[0]) )\n",
    "\n",
    "print(\"BEST MODEL\")\n",
    "\n",
    "print(\"######################################\")\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "for i in d:\n",
    "    print(i,' : ' ,d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
